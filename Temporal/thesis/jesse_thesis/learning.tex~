%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Learning}
\label{sec:learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We now describe the learning approach used, including the algorithm and features. 
For this work, we use an online, error-driven perceptron. A training instance consists of \begin{inparaenum}[\itshape a\upshape)] \item the document (sentences, document publication date), \item the grounding for the previously uttered temproal phrase, \item the current temporal phrase, and \item the pair of the gold type and value.\end{inparaenum} 


First, we parse the current sentence, then chose the optimal logical form and execute it to check against our gold type and value. If the execution doesn't result in the correct type and value, we find the best logical form that does and run an additive, perceptron-style parameter update. 


\begin{description}
  \item[Inputs:]  \hfill \\
Training examples ${I_i|i = 1\ldots n}$. Each $I_i$ is a sequence ${d_j, p_i, c_i, tv_i}$, where $d_j$ is the document for $I_i$, $p_i$ is the result of grounding $I_{i-1}$, $c_i$ is the current phrase, and $tv_i$ is the gold type and value pair. Number of training iteraniots $T$. Initial parameters $\theta$ \\
  \item[Definitions:] \hfill \\
  The function $\phi(d)$ represents the features described in section~\ref{sec:features}. $GEN(w)$ is the set of derivations for phrase $w$. $GEN(w,tv)$ is the set of derivations for phrase $w$ that produce a logical form that executes to type and value pair $tv$. The function $L(d)$ maps a derivation to its associated final executed output.
  \item[Algorithm:] \hfill \\
  \begin{itemize}
    \item For $t = 1\ldots T,i=1\ldots n:$ (Iterations)
    \begin{description}
      \item[Step 1:] (check correctness)
      \begin{itemize}
        \item Let $d*=arg max_{d\in GEN(w_i)}\theta*\phi(d).$
        \item If $L(d*)=tv_i$, go to Step 3.
      \end{itemize}
      \item[Step 2:] (Update parameters)
      \begin{itemize}
        \item Let $d'=arg max_{d\in GEN(w_i,tv_i)}\theta*\phi(d).$
        \item Set $\theta=\theta+\phi(d')-\phi(d*).$
      \end{itemize}
    \end{description}
  \end{itemize}
  \item[Output:] Estimated parameters $\theta$.
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Features}
\label{sec:features}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We use a small set of linguistically motivated features for this work. We have a set of lexical features over the base parse, which are sensitive to the lexical choices and the structure of the logical form that is constructed. We also have a set of features derived from the context in which a given temporal phrase appears. These features use a dependency parse to find the verb that governs the temporal phrase, then look at its POS tag and the POS tag of any other verbs paired with it via a dependency arc. These features represent information on the tense of the govener verb, and help us choose between the different context-dependent logical forms. (See~\ref{fig:features}.) Finally, we have an indicator feature for phrases that need to be ground relative to the previously uttered temporal phrase, such as \emph{a year earlier} or \emph{that quarter}.

\begin{figure}[t!]
   \center
   {\includegraphics[width=0.5\columnwidth]{fig/featureExample.png}}
   \caption{
   When grounding the phrase \emph{third quarter} in the example sentences above, we compute features based on the tense of the verb that governs \emph{third quarter}. These features help us select between the three context-dependent logical forms shown above. 
   } 
   \label{fig:features}
\end{figure}
